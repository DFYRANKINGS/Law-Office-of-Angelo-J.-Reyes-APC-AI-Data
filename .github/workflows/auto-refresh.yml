name: "🔄 Auto Refresh AI Files (Every 4 Weeks If Stale + Changed)"

on:
  schedule:
    - cron: "0 4 * * 0"     # Every Sunday at 04:00 UTC
  workflow_dispatch:         # Allow manual run

permissions:
  contents: write

jobs:
  refresh:
    runs-on: ubuntu-latest
    steps:
      - name: 📥 Checkout Repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0

      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: 📦 Install Dependencies
        run: |
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            python -m pip install --upgrade pip
            pip install pyyaml pandas openpyxl
          fi

      - name: 🕰️ Check If Sitemap Is Older Than 28 Days
        id: check_stale
        run: |
          if [ -f "ai-sitemap.xml" ]; then
            FILE_MTIME=$(stat -c %Y ai-sitemap.xml)
            NOW=$(date +%s)
            DIFF_DAYS=$(( (NOW - FILE_MTIME) / 86400 ))
            echo "📄 ai-sitemap.xml last modified $DIFF_DAYS days ago."
            if [ $DIFF_DAYS -ge 28 ]; then
              echo "is_stale=true" >> $GITHUB_ENV
              echo "✅ File is stale — proceeding"
            else
              echo "is_stale=false" >> $GITHUB_ENV
              echo "❌ File is fresh — skipping auto-refresh"
            fi
          else
            echo "⚠️ ai-sitemap.xml not found — treating as stale"
            echo "is_stale=true" >> $GITHUB_ENV
          fi

      - name: 🔄 Skip If Not Stale
        if: env.is_stale != 'true'
        run: |
          echo "✅ Skipping — recent activity detected."
          exit 0

      # 1) Re-generate schema files from XLSX (adjust input path if needed)
      - name: 🗃️ Re-generate Schema Files
        run: |
          python ai-generators/generate_files_from_xlsx.py --input templates/AI-Visibility-Master-Template.xlsx

      # 2) Build public HTML pages from schemas
      - name: 🧱 Build Public Pages
        run: |
          python ai-generators/build_public_pages.py

      # 3) Generate both sitemaps (HTML + AI) — include all root *.html
      - name: 🗺️ Re-generate Sitemaps
        run: |
          python generate_sitemaps.py --include-all-html
          # touch not needed; generate_sitemaps.py writes files explicitly

      # 4) Generate robots.txt that adapts to repo/branch automatically
      - name: 🤖 Generate robots.txt
        run: |
          python generate_robots.py

      # 5) Detect changes across everything we just produced
      - name: 🧪 Detect Real Changes
        id: detect_changes
        run: |
          git add --all
          if ! git diff --cached --quiet; then
            echo "changes_detected=true" >> $GITHUB_ENV
            echo "✅ Real changes detected — will commit and ping"
          else
            echo "changes_detected=false" >> $GITHUB_ENV
            echo "✅ No real changes — skipping commit and ping"
          fi

      # 6) Commit once if there are changes (schemas, pages, sitemaps, robots)
      - name: 💾 Commit Only If Changed
        if: env.changes_detected == 'true'
        run: |
          git config --local user.name  "github-actions[bot]"
          git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git commit -m "🔄 Auto-refresh: content regenerated after 28+ days idle ($(date +'%Y-%m-%d'))"
          git push origin ${{ github.ref_name || 'main' }}
          echo "✅ Changes pushed"

      # 7) Ping search engines (only if we pushed changes)
      - name: 📡 Ping Search Engines (ONLY if changes committed)
        if: env.changes_detected == 'true'
        run: |
          OWNER_REPO="${{ github.repository }}"
          OWNER="${OWNER_REPO%/*}"
          REPO="${OWNER_REPO#*/}"
          BRANCH="${{ github.ref_name || 'main' }}"

          # Project Pages base: https://<owner>.github.io/<repo>
          PAGES_BASE="https://${OWNER}.github.io/${REPO}"

          # If CNAME exists, prefer it (generate_sitemaps.py uses it too)
          if [ -f CNAME ]; then
            HOST=$(cat CNAME | tr -d '\r' | sed 's#http://##; s#https://##' | tr -d '/')
            if [ -n "$HOST" ]; then
              PAGES_BASE="https://${HOST}"
            fi
          fi

          PAGE_SITEMAP="${PAGES_BASE}/sitemap.xml"
          AI_SITEMAP="https://raw.githubusercontent.com/${OWNER}/${REPO}/${BRANCH}/ai-sitemap.xml"

          echo "📄 Page sitemap: ${PAGE_SITEMAP}"
          echo "🤖 AI sitemap:   ${AI_SITEMAP}"

          echo "→ Notifying Google (pages sitemap)..."
          curl -s -o /dev/null -w "%{http_code}\n" "https://www.google.com/ping?sitemap=${PAGE_SITEMAP}" || true

          echo "→ Notifying Bing (pages sitemap)..."
          curl -s -o /dev/null -w "%{http_code}\n" "https://www.bing.com/ping?sitemap=${PAGE_SITEMAP}" || true

          # Optional: some engines may also accept the AI sitemap URL; harmless to try
          echo "→ (Optional) Notifying Google (AI sitemap)..."
          curl -s -o /dev/null -w "%{http_code}\n" "https://www.google.com/ping?sitemap=${AI_SITEMAP}" || true

          echo "→ (Optional) Notifying Bing (AI sitemap)..."
          curl -s -o /dev/null -w "%{http_code}\n" "https://www.bing.com/ping?sitemap=${AI_SITEMAP}" || true
